{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain-nomic langchain_community tiktoken chromadb langchainhub langchain langgraph tavily-python gpt4all firecrawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Take environment variables from .env.\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANG_KEY') # replace with your own key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = 'llama3' # Using llama3 but you can use anything you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "import requests\n",
    "\n",
    "# Bunch of randomly generated URLs (restricting to 3 to not destroy API token limits)\n",
    "urls = {\n",
    "    'https://en.wikipedia.org/wiki/Knowledge_graph',\n",
    "    'https://en.wikipedia.org/wiki/Semantic_technology',\n",
    "    'https://en.wikipedia.org/wiki/Semantic_integration'\n",
    "    # 'https://en.wikipedia.org/wiki/Logical_graph',\n",
    "    # 'https://en.wikipedia.org/wiki/Knowledge_graph_embedding',\n",
    "    # 'https://en.wikipedia.org/wiki/Graph_database',\n",
    "    # 'https://en.wikipedia.org/wiki/Formal_semantics_(natural_language)',\n",
    "    # 'https://en.wikipedia.org/wiki/Artificial_general_intelligence',\n",
    "    # 'https://en.wikipedia.org/wiki/Recursive_self-improvement',\n",
    "    # 'https://en.wikipedia.org/wiki/Automated_planning_and_scheduling',\n",
    "    # 'https://en.wikipedia.org/wiki/Machine_learning',\n",
    "    # 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "   'Accept': 'application/json',\n",
    "   'Authorization': os.getenv('JINA_KEY') # replace with your own api key \n",
    "}\n",
    "\n",
    "base_url = 'https://r.jina.ai/'\n",
    "\n",
    "docs = [requests.get(base_url+url, headers=headers).json() for url in urls]\n",
    "\n",
    "docs_list = []\n",
    "\n",
    "# Look up JINA API response format but essentially we are extracting the content and reconstructing metadata from the response\n",
    "for doc in docs:\n",
    "    metadata = {k: v for k, v in doc['data'].items() if k != 'content'}\n",
    "    docs_list.append({\"content\": doc['data']['content'], \"metadata\": metadata})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document into smaller chunks. Smaller chunk sizes are usually better (not too small) but your results will vary depending on the prompt and your local data. Will take longer to index if chunk size is small but will potentially alleviate loss-in-the-middle issues.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter().from_tiktoken_encoder(\n",
    "    chunk_size=256, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(texts=[doc['content'] for doc in docs_list], metadatas=[doc['metadata'] for doc in docs_list])\n",
    "\n",
    "\n",
    "# Filter out metadata that comes as an array and restrict it to just primitive types\n",
    "filtered_docs = []\n",
    "\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        if doc.metadata is not None:\n",
    "            clean_metadata = {k: v for k, v in doc.metadata.items() if type(v) in [str, int, float, bool]}\n",
    "        else:\n",
    "            clean_metadata = {}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vector DB\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding = GPT4AllEmbeddings(\n",
    "        model_name=model_name,\n",
    "        gpt4all_kwargs=gpt4all_kwargs\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define a retriever from our vectorstore\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now create a retrieval grader to determine if the document pulled is relevant to the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'no'}\n",
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatOllama(model=local_llm, format='json', temperature=0)\n",
    "\n",
    "# The following prompt is generally how you would structure a retrieval grader prompt with roles defined between header_ids\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and a premable or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question_right = \"What is a knowledge graph?\" # Test question to see if it can identify a relevant document from our store\n",
    "docs_right = retriever.invoke(question)\n",
    "doc_text = docs_right[1].page_content # We take a sample document from the retrieved documents\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_text})) # Now we check to see if that doc is relevant\n",
    "\n",
    "question_wrong = \"Who made sesame street?\"\n",
    "docs_wrong = retriever.invoke(question)\n",
    "doc_text = docs_wrong[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_text})) # Now we check to see if that doc is relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming the above was relevant, let's now generate our response with the document that was retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesame Street was created by Joan Ganz Cooney and Lloyd Morris. They developed the concept for the show, which premiered in 1969, with the goal of using television to educate young children about letters, numbers, and social skills.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant tasked with generating a response to a user question. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use a maximum of 3 sentences and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the user question: {question} \n",
    "    Here is the context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"Who made?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"question\": question, \"context\": docs})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the event that the retrieved document is not relevant, let's opt for a web search via Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
