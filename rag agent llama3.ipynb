{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\n",
      "  Downloading langchain_nomic-0.1.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.17-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.0.57-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.3.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gpt4all\n",
      "  Downloading gpt4all-2.6.0-py3-none-macosx_10_15_universal2.whl.metadata (4.1 kB)\n",
      "Collecting firecrawl-py\n",
      "  Downloading firecrawl_py-0.0.11-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-core<0.3,>=0.1.46 (from langchain-nomic)\n",
      "  Downloading langchain_core-0.2.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting nomic<4.0.0,>=3.0.29 (from langchain-nomic)\n",
      "  Downloading nomic-3.0.29.tar.gz (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PyYAML>=5.3 (from langchain_community)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Downloading SQLAlchemy-2.0.30-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.65-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain_community)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting requests<3,>=2 (from langchain_community)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain_community)\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pydantic>=1.9 (from chromadb)\n",
      "  Using cached pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.30.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.conda/lib/python3.11/site-packages (from chromadb) (4.11.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.18.0-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.2 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm>=4.65.0 (from chromadb)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.64.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.1.3-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.5 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.3-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uuid6<2025.0.0,>=2024.1.12 (from langgraph)\n",
      "  Downloading uuid6-2024.1.12-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in ./.conda/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting httpx>=0.23.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting jinja2>=2.11.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading ujson-5.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.3 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting certifi>=14.05.14 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.46->langchain-nomic)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging>=19.1 (from build>=1.0.3->chromadb)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jsonlines (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting rich (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pandas (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting pyarrow (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading pyarrow-16.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pillow (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached pillow-10.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyjwt (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.27.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.conda/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.5.1)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.9->chromadb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.3 (from pydantic>=1.9->chromadb)\n",
      "  Downloading pydantic_core-2.18.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain_community)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain_community)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.22.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting anyio (from httpx>=0.23.0->fastapi>=0.95.2->chromadb)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi>=0.95.2->chromadb)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sniffio (from httpx>=0.23.0->fastapi>=0.95.2->chromadb)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.11/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-nomic)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic) (2.18.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading langchain_nomic-0.1.1-py3-none-any.whl (3.8 kB)\n",
      "Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m907.0/907.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl (198 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.7/198.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchainhub-0.1.17-py3-none-any.whl (4.8 kB)\n",
      "Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-0.0.57-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n",
      "Downloading gpt4all-2.6.0-py3-none-macosx_10_15_universal2.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading firecrawl_py-0.0.11-py3-none-any.whl (6.2 kB)\n",
      "Downloading aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.1.3-cp39-abi3-macosx_10_12_universal2.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.64.0-cp311-cp311-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.2-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.65-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Downloading onnxruntime-1.18.0-cp311-cp311-macosx_11_0_universal2.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
      "Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.3-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (253 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.6/253.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "Downloading pydantic_core-2.18.3-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.30-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
      "Downloading uuid6-2024.1.12-py3-none-any.whl (6.4 kB)\n",
      "Downloading uvicorn-0.30.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl (145 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.9/145.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading ujson-5.10.0-cp311-cp311-macosx_11_0_arm64.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.22.0-cp311-cp311-macosx_11_0_arm64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.2/391.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-10.3.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Downloading pyarrow-16.1.0-cp311-cp311-macosx_11_0_arm64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: nomic, pypika\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomic: filename=nomic-3.0.29-py3-none-any.whl size=44793 sha256=caf3030e042557ea5ad481ed92e76c5e3d1022c68fc0fe9ffcfa3cc93af82383\n",
      "  Stored in directory: /Users/dhruvareddy/Library/Caches/pip/wheels/f8/e7/ac/53f4833a5cc144a45086515cf2052232f116e0fc9103ff2be1\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=084466315b4c3bb5f71814a8d1601f9676f9791a95db3b82f5abb2be69d8bfe9\n",
      "  Stored in directory: /Users/dhruvareddy/Library/Caches/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built nomic pypika\n",
      "Installing collected packages: pytz, pypika, mpmath, monotonic, mmh3, flatbuffers, wrapt, websockets, websocket-client, uvloop, uuid6, urllib3, ujson, tzdata, tqdm, tenacity, sympy, SQLAlchemy, sniffio, shellingham, regex, PyYAML, python-multipart, python-dotenv, pyproject_hooks, pyjwt, pydantic-core, pyasn1, protobuf, pillow, packaging, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, numpy, mypy-extensions, multidict, mdurl, MarkupSafe, loguru, jsonpointer, importlib-resources, importlib-metadata, idna, humanfriendly, httptools, h11, grpcio, fsspec, frozenlist, filelock, dnspython, click, charset-normalizer, certifi, cachetools, bcrypt, backoff, attrs, asgiref, annotated-types, yarl, uvicorn, typing-inspect, types-requests, rsa, requests, pydantic, pyasn1-modules, pyarrow, pandas, opentelemetry-proto, marshmallow, markdown-it-py, jsonpatch, jsonlines, jinja2, httpcore, googleapis-common-protos, email_validator, deprecated, coloredlogs, chroma-hnswlib, build, anyio, aiosignal, watchfiles, tiktoken, starlette, rich, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, langsmith, langchainhub, huggingface-hub, httpx, gpt4all, google-auth, firecrawl-py, dataclasses-json, aiohttp, typer, tokenizers, tavily-python, opentelemetry-sdk, opentelemetry-instrumentation, nomic, langchain-core, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langgraph, langchain-text-splitters, langchain-nomic, fastapi-cli, opentelemetry-instrumentation-fastapi, langchain, fastapi, langchain_community, chromadb\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 7.1.0\n",
      "    Uninstalling importlib_metadata-7.1.0:\n",
      "      Successfully uninstalled importlib_metadata-7.1.0\n",
      "Successfully installed MarkupSafe-2.1.5 PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 asgiref-3.8.1 attrs-23.2.0 backoff-2.2.1 bcrypt-4.1.3 build-1.2.1 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 chroma-hnswlib-0.7.3 chromadb-0.5.0 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.6 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 filelock-3.14.0 firecrawl-py-0.0.11 flatbuffers-24.3.25 frozenlist-1.4.1 fsspec-2024.5.0 google-auth-2.29.0 googleapis-common-protos-1.63.0 gpt4all-2.6.0 grpcio-1.64.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.2 humanfriendly-10.0 idna-3.7 importlib-metadata-7.0.0 importlib-resources-6.4.0 jinja2-3.1.4 jsonlines-4.0.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.2.1 langchain-core-0.2.2 langchain-nomic-0.1.1 langchain-text-splitters-0.2.0 langchain_community-0.2.1 langchainhub-0.1.17 langgraph-0.0.57 langsmith-0.1.65 loguru-0.7.2 markdown-it-py-3.0.0 marshmallow-3.21.2 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 multidict-6.0.5 mypy-extensions-1.0.0 nomic-3.0.29 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.18.0 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.3 overrides-7.7.0 packaging-23.2 pandas-2.2.2 pillow-10.3.0 posthog-3.5.0 protobuf-4.25.3 pyarrow-16.1.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.7.2 pydantic-core-2.18.3 pyjwt-2.8.0 pypika-0.48.9 pyproject_hooks-1.1.0 python-dotenv-1.0.1 python-multipart-0.0.9 pytz-2024.1 regex-2024.5.15 requests-2.32.3 requests-oauthlib-2.0.0 rich-13.7.1 rsa-4.9 shellingham-1.5.4 sniffio-1.3.1 starlette-0.37.2 sympy-1.12.1 tavily-python-0.3.3 tenacity-8.3.0 tiktoken-0.7.0 tokenizers-0.19.1 tqdm-4.66.4 typer-0.12.3 types-requests-2.32.0.20240523 typing-inspect-0.9.0 tzdata-2024.1 ujson-5.10.0 urllib3-2.2.1 uuid6-2024.1.12 uvicorn-0.30.0 uvloop-0.19.0 watchfiles-0.22.0 websocket-client-1.8.0 websockets-12.0 wrapt-1.16.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain-nomic langchain_community tiktoken chromadb langchainhub langchain langgraph tavily-python gpt4all firecrawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "langchain_api_key = os.getenv('LANG_KEY') # replace with your own key\n",
    "jina_key = os.getenv('JINA_KEY') # replace with your own key \n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = 'llama3' #Using llama3 but you can use anything you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "import requests\n",
    "\n",
    "#bunch of randomly generated URLs (restricting to 3 to not destroy API token limits)\n",
    "\n",
    "urls = {\n",
    "    'https://en.wikipedia.org/wiki/Knowledge_graph',\n",
    "    'https://en.wikipedia.org/wiki/Semantic_technology',\n",
    "    'https://en.wikipedia.org/wiki/Semantic_integration'\n",
    "    # 'https://en.wikipedia.org/wiki/Logical_graph',\n",
    "    # 'https://en.wikipedia.org/wiki/Knowledge_graph_embedding',\n",
    "    # 'https://en.wikipedia.org/wiki/Graph_database',\n",
    "    # 'https://en.wikipedia.org/wiki/Formal_semantics_(natural_language)',\n",
    "    # 'https://en.wikipedia.org/wiki/Artificial_general_intelligence',\n",
    "    # 'https://en.wikipedia.org/wiki/Recursive_self-improvement',\n",
    "    # 'https://en.wikipedia.org/wiki/Automated_planning_and_scheduling',\n",
    "    # 'https://en.wikipedia.org/wiki/Machine_learning',\n",
    "    # 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "   'Accept': 'application/json',\n",
    "   'Authorization': jina_key\n",
    "}\n",
    "\n",
    "base_url = 'https://r.jina.ai/'\n",
    "\n",
    "docs = [requests.get(base_url+url, headers=headers).json() for url in urls]\n",
    "\n",
    "docs_list = []\n",
    "\n",
    "#look up JINA API response format but essentially we are extracting the content and reconstructing metadata from the response\n",
    "for doc in docs:\n",
    "    metadata = {k: v for k, v in doc['data'].items() if k != 'content'}\n",
    "    docs_list.append({\"content\": doc['data']['content'], \"metadata\": metadata})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='[![Image 1: Page semi-protected](https://upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png)](https://en.wikipedia.org/wiki/Wikipedia:Protection_policy#semi \"This article is semi-protected.\")\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n**Semantic integration** is the process of interrelating information from diverse sources, for example calendars and to do lists, email archives, presence information (physical, psychological, and social), documents of all sorts, contacts (including [social graphs](https://en.wikipedia.org/wiki/Social_graph \"Social graph\")), search results, and advertising and marketing relevance derived from them. In this regard, [semantics](https://en.wikipedia.org/wiki/Semantics \"Semantics\") focuses on the organization of and action upon [information](https://en.wikipedia.org/wiki/Information \"Information\") by acting as an intermediary between heterogeneous data sources, which may conflict not only by structure but also context or value.', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='Applications and methods\\n------------------------', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='In [enterprise application integration](https://en.wikipedia.org/wiki/Enterprise_application_integration \"Enterprise application integration\") (EAI), semantic integration can facilitate or even automate the communication between computer systems using [metadata publishing](https://en.wikipedia.org/wiki/Metadata_publishing \"Metadata publishing\"). Metadata publishing potentially offers the ability to automatically link [ontologies](https://en.wikipedia.org/wiki/Ontology_\\\\(computer_science\\\\) \"Ontology (computer science)\"). One approach to (semi-)automated ontology mapping requires the definition of a semantic distance or its inverse, [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity \"Semantic similarity\") and appropriate rules. Other approaches include so-called _lexical methods_, as well as methodologies that rely on exploiting the structures of the ontologies. For explicitly stating similarity/equality, there exist special properties or relationships in most ontology languages. [OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language \"Web Ontology Language\"), for example has \"owl:equivalentClass\", \"owl:equivalentProperty\"', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='and \"owl:[sameAs](https://en.wikipedia.org/wiki/SameAs \"SameAs\")\".', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='Eventually system designs may see the advent of composable architectures where published semantic-based interfaces are joined together to enable new and meaningful capabilities\\\\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation needed\")_\\\\]. These could predominately be described by means of design-time declarative specifications, that could ultimately be rendered and executed at run-time\\\\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation needed\")_\\\\].', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='Semantic integration can also be used to facilitate design-time activities of interface design and mapping. In this model, semantics are only explicitly applied to design and the run-time systems work at the [syntax](https://en.wikipedia.org/wiki/Syntax \"Syntax\") level\\\\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation needed\")_\\\\]. This \"early semantic binding\" approach can improve overall system performance while retaining the benefits of semantic driven design\\\\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation needed\")_\\\\].\\n\\nSemantic integration situations\\n-------------------------------', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='From the industry use case, it has been observed that the semantic mappings were performed only within the scope of the ontology class or the [datatype](https://en.wikipedia.org/wiki/Data_type \"Data type\") property. These identified semantic integrations are (1) integration of ontology class instances into another ontology class without any constraint, (2) integration of selected instances in one ontology class into another ontology class by the range constraint of the property value and (3) integration of ontology class instances into another ontology class with the value transformation of the instance property. Each of them requires a particular mapping relationship, which is respectively: (1) equivalent or subsumption mapping relationship, (2) conditional mapping relationship that constraints the value of property (data range) and (3) transformation mapping relationship that transforms the value of property (unit transformation). Each identified mapping relationship can be defined as either (1) direct mapping type, (2) data range mapping type or (3) unit transformation mapping type.\\n\\nKG vs. RDB approaches\\n---------------------\\n\\nIn the case of integrating supplemental data source,', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='*   KG([Knowledge graph](https://en.wikipedia.org/wiki/Knowledge_graph \"Knowledge graph\")) formally represents the meaning involved in information by describing concepts, relationships between things, and categories of things. These embedded semantics with the data offer significant advantages such as reasoning over data and dealing with heterogeneous data sources. The rules can be applied on KG more efficiently using graph query. For example, the graph query does the data inference through the connected relations, instead of repeated full search of the tables in relational database. KG facilitates the integration of new heterogeneous data by just adding new relationships between existing information and new entities. This facilitation is emphasized for the integration with existing popular linked open data source such as Wikidata.org.', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='*   [SQL](https://en.wikipedia.org/wiki/SQL \"SQL\") query is tightly coupled and rigidly constrained by datatype within the specific database and can join tables and extract data from tables, and the result is generally a table, and a query can join tables by any columns which match by datatype. [SPARQL](https://en.wikipedia.org/wiki/SPARQL \"SPARQL\") query is the standard query language and protocol for Linked Open Data on the web and loosely coupled with the database so that it facilitates the reusability and can extract data through the relations free from the datatype, and not only extract but also generate additional knowledge graph with more sophisticated operations(logic: transitive/symmetric/inverseOf/functional). The inference based query (query on the existing asserted facts without the generation of new facts by logic) can be fast comparing to the reasoning based query (query on the existing plus the generated/discovered facts based on logic).', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='*   The information integration of heterogeneous data sources in traditional database is intricate, which requires the redesign of the database table such as changing the structure and/or addition of new data. In the case of semantic query, SPARQL query reflects the relationships between entities in a way that aligned with human\\'s understanding of the domain, so the semantic intention of the query can be seen on the query itself. Unlike SPARQL, SQL query, which reflects the specific structure of the database and derived from matching the relevant primary and foreign keys of tables, loses the semantics of the query by missing the relationships between entities. Below is the example that compares SPARQL and SQL queries for medications that treats \"TB of vertebra\".', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='> SELECT ?medication  \\n> WHERE {  \\n> ?diagnosis a example:Diagnosis .  \\n> ?diagnosis example:name “TB of vertebra” .  \\n> ?medication example:canTreat ?diagnosis .  \\n> }\\n\\n> SELECT DRUG.medID  \\n> FROM DIAGNOSIS, DRUG, DRUG\\\\_DIAGNOSIS  \\n> WHERE DIAGNOSIS.diagnosisID=DRUG\\\\_DIAGNOSIS.diagnosisID  \\n> AND DRUG.medID=DRUG\\\\_DIAGNOSIS.medID  \\n> AND DIAGNOSIS.name=”TB of vertebra”\\n\\nExamples\\n--------\\n\\nThe [Pacific Symposium on Biocomputing](https://en.wikipedia.org/wiki/Pacific_Symposium_on_Biocomputing \"Pacific Symposium on Biocomputing\") has been a venue for the popularization of the ontology mapping task in the biomedical domain, and a number of papers on the subject can be found in its proceedings.', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='See also\\n--------', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='*   [Data integration](https://en.wikipedia.org/wiki/Data_integration \"Data integration\")\\n*   [Dataspaces](https://en.wikipedia.org/wiki/Dataspaces \"Dataspaces\")\\n*   [Enterprise integration](https://en.wikipedia.org/wiki/Enterprise_integration \"Enterprise integration\")\\n*   [Ontology-based data integration](https://en.wikipedia.org/wiki/Ontology-based_data_integration \"Ontology-based data integration\")\\n*   [Ontology alignment](https://en.wikipedia.org/wiki/Ontology_alignment \"Ontology alignment\")\\n*   [Ontology engineering](https://en.wikipedia.org/wiki/Ontology_engineering \"Ontology engineering\")\\n*   [Ontology matching](https://en.wikipedia.org/wiki/Ontology_matching \"Ontology matching\")\\n*   [Semantic heterogeneity](https://en.wikipedia.org/wiki/Semantic_heterogeneity \"Semantic heterogeneity\")', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='*   [Semantic technology](https://en.wikipedia.org/wiki/Semantic_technology \"Semantic technology\")\\n*   [Semantic translation](https://en.wikipedia.org/wiki/Semantic_translation \"Semantic translation\")\\n*   [Semantic unification](https://en.wikipedia.org/wiki/Semantic_unification \"Semantic unification\")', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='References\\n----------\\n\\nExternal links\\n--------------\\n\\n*   [Semantic Integration: Loosely Coupling the Meaning of Data](https://web.archive.org/web/20070811204850/http://zapthink.com/report.html?id=ZapFlash-08082003)\\n*   [Ontology Mapping: The State of the Art](http://drops.dagstuhl.de/opus/volltexte/2005/40/) (2005 paper)\\n*   [2010 paper by Carl Hewitt](https://arxiv.org/ftp/arxiv/papers/0901/0901.4934.pdf)\\n*   [OpenCyc to Oracle Interface](https://web.archive.org/web/20160312203727/http://wwwhome.portavita.nl/~yeb/ooi.pdf)', metadata={'title': 'Semantic integration', 'url': 'https://en.wikipedia.org/wiki/Semantic_integration', 'publishedTime': '2006-03-13T21:08:41Z'}), Document(page_content='From Wikipedia, the free encyclopedia\\n\\n[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/SemanticNetExample.jpg/220px-SemanticNetExample.jpg)](https://en.wikipedia.org/wiki/File:SemanticNetExample.jpg)\\n\\nSimplistic example of the sort of [semantic net](https://en.wikipedia.org/wiki/Semantic_net \"Semantic net\") used in [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web \"Semantic Web\") technology', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='The ultimate goal of **semantic technology** is to help machines understand data. To enable the encoding of semantics with the data, well-known technologies are [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework \"Resource Description Framework\") (Resource Description Framework)[\\\\[1\\\\]](#cite_note-1) and [OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language \"Web Ontology Language\") (Web Ontology Language).[\\\\[2\\\\]](#cite_note-2) These technologies formally [represent](https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning \"Knowledge representation and reasoning\") the meaning involved in information. For example, [ontology](https://en.wikipedia.org/wiki/Ontology_\\\\(computer_science\\\\) \"Ontology (computer science)\") can describe concepts, relationships between things, and categories of things. These embedded semantics with the data offer significant advantages such as reasoning over data and dealing with heterogeneous data sources.', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='Overview\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Semantic_technology&action=edit&section=1 \"Edit section: Overview\")\\\\]\\n---------------------------------------------------------------------------------------------------------------------------------\\n\\nIn [software](https://en.wikipedia.org/wiki/Software \"Software\"), semantic technology encodes meanings separately from data and content files, and separately from application code. This enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.\\n\\nWith traditional [information technology](https://en.wikipedia.org/wiki/Information_technology \"Information technology\"), on the other hand, meanings and relationships must be predefined and \"hard wired\" into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='Off-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.\\n\\nSemantic technologies are \"meaning-centered\". They involve but are not limited to the following areas of application:\\n\\n*   encoding/decoding of semantic representation,\\n*   [knowledge graphs](https://en.wikipedia.org/wiki/Knowledge_graph \"Knowledge graph\") of entities and their interrelationships,\\n*   auto-recognition of topics and concepts,\\n*   [information and meaning extraction](https://en.wikipedia.org/wiki/Information_extraction \"Information extraction\"),\\n*   semantic data integration, and\\n*   taxonomies/classification.\\n\\nGiven a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone. Semantic technologies would often leverage natural language processing and machine learning in order to extract topics, concepts, and associations between concepts in text.\\n\\nSee also\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Semantic_technology&action=edit&section=2 \"Edit section: See also\")\\\\]\\n---------------------------------------------------------------------------------------------------------------------------------', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='*   [Knowledge graph](https://en.wikipedia.org/wiki/Knowledge_graph \"Knowledge graph\")\\n*   [Metadata](https://en.wikipedia.org/wiki/Metadata \"Metadata\")\\n*   [Ontology](https://en.wikipedia.org/wiki/Ontology_\\\\(information_science\\\\) \"Ontology (information science)\")\\xa0– also known as a [knowledge graph](https://en.wikipedia.org/wiki/Knowledge_graph \"Knowledge graph\") in a generalized term\\n*   [Resource Description Framework](https://en.wikipedia.org/wiki/Resource_Description_Framework \"Resource Description Framework\")\\n*   [Schema.org](https://en.wikipedia.org/wiki/Schema.org \"Schema.org\")\\xa0– a set of schemas for structured data markup on web pages\\n*   [Semantic heterogeneity](https://en.wikipedia.org/wiki/Semantic_heterogeneity \"Semantic heterogeneity\")\\n*   [Semantic integration](https://en.wikipedia.org/wiki/Semantic_integration \"Semantic integration\")', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='*   [Semantic matching](https://en.wikipedia.org/wiki/Semantic_matching \"Semantic matching\")\\n*   [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web \"Semantic Web\")\\n*   [Web Ontology Language](https://en.wikipedia.org/wiki/Web_Ontology_Language \"Web Ontology Language\")', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='References\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Semantic_technology&action=edit&section=3 \"Edit section: References\")\\\\]\\n-------------------------------------------------------------------------------------------------------------------------------------\\n\\n1.  **[^](#cite_ref-1 \"Jump up\")** [\"World Wide Web Consortium (W3C), \"RDF/XML Syntax Specification (Revised)\", 10 Feb. 2004\"](http://www.w3.org/TR/rdf-syntax-grammar/).\\n2.  **[^](#cite_ref-2 \"Jump up\")** [\"World Wide Web Consortium (W3C), \"OWL Web Ontology Language Overview\", W3C Recommendation, 10 Feb. 2004\"](http://www.w3.org/TR/owl-features/).\\n\\nFurther reading\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Semantic_technology&action=edit&section=4 \"Edit section: Further reading\")\\\\]\\n-----------------------------------------------------------------------------------------------------------------------------------------------', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='*   J.T. Pollock, R. Hodgson. _[Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration](https://books.google.com/books?id=NHSDrsizjtEC)._ [John Wiley & Sons](https://en.wikipedia.org/wiki/John_Wiley_%26_Sons \"John Wiley & Sons\"), October 2004\\n*   R. Guha, R. McCool, and E. Miller. Semantic search. In _WWW2003 — Proc. of the 12th international conference on World Wide Web_, pp 700–709. [ACM Press](https://en.wikipedia.org/wiki/ACM_Press \"ACM Press\"), 2003.\\n*   I. Polikoff and D. Allemang. [Semantic technology.](https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf) _TopQuadrant Technology Briefing_ v1.1, September 2003.', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='*   [T. Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee \"Tim Berners-Lee\"), J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. _[Scientific American](https://en.wikipedia.org/wiki/Scientific_American \"Scientific American\")_, May 2001.\\n*   A.P. Sheth, C. Ramakrishnan. [Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.](http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis.) _IEEE Data Engineering Bulletin_, 2003.\\n*   Steffen Staab, Rudi Studer (Ed.), _Handbook on Ontologies_, Springer,', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='*   Mills Davis. [The Business Value of Semantic Technologies](https://project10x.com/bio_downloads/business_value_of_semantic_technologies_2005.pdf). Presentation and Report. Semantic Technologies for E-Government, September 2004.\\n*   [Pascal Hitzler](https://en.wikipedia.org/wiki/Pascal_Hitzler \"Pascal Hitzler\"), Markus Krötzsch, Sebastian Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, [ISBN](https://en.wikipedia.org/wiki/ISBN_\\\\(identifier\\\\) \"ISBN (identifier)\")\\xa0[978-1-4200-9050-5](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4200-9050-5 \"Special:BookSources/978-1-4200-9050-5\")', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='*   Milošević, Nikola, and Wolfgang Thielemann. [\"Comparison of biomedical relationship extraction methods and models for knowledge graph creation.\"](https://arxiv.org/pdf/2201.01647.pdf) Journal of Semantic Web (JoWS) (2022). Elsevier, [doi:10.1016/j.websem.2022.100756](https://doi.org/10.1016/j.websem.2022.100756 \"doi:10.1016/j.websem.2022.100756\")', metadata={'title': 'Semantic technology', 'url': 'https://en.wikipedia.org/wiki/Semantic_technology', 'publishedTime': '2006-03-16T22:59:41Z'}), Document(page_content='From Wikipedia, the free encyclopedia\\n\\n[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Conceptual_Diagram_-_Example.svg/220px-Conceptual_Diagram_-_Example.svg.png)](https://en.wikipedia.org/wiki/File:Conceptual_Diagram_-_Example.svg)\\n\\nExample conceptual diagram', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='In [knowledge representation and reasoning](https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning \"Knowledge representation and reasoning\"), a **knowledge graph** is a [knowledge base](https://en.wikipedia.org/wiki/Knowledge_base \"Knowledge base\") that uses a [graph](https://en.wikipedia.org/wiki/Graph_\\\\(discrete_mathematics\\\\) \"Graph (discrete mathematics)\")\\\\-structured [data model](https://en.wikipedia.org/wiki/Data_model \"Data model\") or [topology](https://en.wikipedia.org/wiki/Topology \"Topology\") to represent and operate on [data](https://en.wikipedia.org/wiki/Data \"Data\"). Knowledge graphs are often used to store interlinked descriptions of [entities](https://en.wikipedia.org/wiki/Named_entity \"Named entity\")\\xa0– objects, events, situations or abstract concepts\\xa0– while also encoding the [semantics](https://en.wikipedia.org/wiki/Semantics \"Semantics\") or relationships underlying these', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='entities.[\\\\[1\\\\]](#cite_note-1)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Since the development of the [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web \"Semantic Web\"), knowledge graphs have often been associated with [linked open data](https://en.wikipedia.org/wiki/Linked_data \"Linked data\") projects, focusing on the connections between [concepts](https://en.wikipedia.org/wiki/Concept \"Concept\") and entities.[\\\\[2\\\\]](#cite_note-Ref1-2)[\\\\[3\\\\]](#cite_note-3) They are also historically associated with and used by [search engines](https://en.wikipedia.org/wiki/Search_engine \"Search engine\") such as [Google](https://en.wikipedia.org/wiki/Google_Knowledge_Graph \"Google Knowledge Graph\"), [Bing](https://en.wikipedia.org/wiki/Bing_\\\\(search_engine\\\\) \"Bing (search engine)\"), [Yext](https://en.wikipedia.org/wiki/Yext \"Yext\") and [Yahoo](https://en.wikipedia.org/wiki/Yahoo \"Yahoo\");', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='[knowledge-engines](https://en.wikipedia.org/wiki/Knowledge_Engine_\\\\(Wikimedia_Foundation\\\\) \"Knowledge Engine (Wikimedia Foundation)\") and question-answering services such as [WolframAlpha](https://en.wikipedia.org/wiki/WolframAlpha \"WolframAlpha\"), Apple\\'s [Siri](https://en.wikipedia.org/wiki/Siri \"Siri\"), and Amazon [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa \"Amazon Alexa\"); and [social networks](https://en.wikipedia.org/wiki/Social_network \"Social network\") such as [LinkedIn](https://en.wikipedia.org/wiki/LinkedIn \"LinkedIn\") and [Facebook](https://en.wikipedia.org/wiki/Facebook \"Facebook\").', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Recent developments in data science and machine learning, particularly in graph neural networks and representation learning, have broadened the scope of knowledge graphs beyond their traditional use in search engines and recommender systems. They are increasingly used in scientific research, with notable applications in fields such as genomics, proteomics, and systems biology.[\\\\[4\\\\]](#cite_note-4)\\n\\nHistory\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=1 \"Edit section: History\")\\\\]\\n---------------------------------------------------------------------------------------------------------------------------', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='The term was coined as early as 1972 by the Austrian [linguist](https://en.wikipedia.org/wiki/Linguistics \"Linguistics\") [Edgar W. Schneider](https://en.wikipedia.org/wiki/Edgar_W._Schneider \"Edgar W. Schneider\"), in a discussion of how to build modular instructional systems for courses.[\\\\[5\\\\]](#cite_note-5) In the late 1980s, the [University of Groningen](https://en.wikipedia.org/wiki/University_of_Groningen \"University of Groningen\") and [University of Twente](https://en.wikipedia.org/wiki/University_of_Twente \"University of Twente\") jointly began a project called Knowledge Graphs, focusing on the design of [semantic networks](https://en.wikipedia.org/wiki/Semantic_network \"Semantic network\") with edges restricted to a limited set of relations, to facilitate [algebras on the graph](https://en.wikipedia.org/wiki/Graph_algebra \"Graph algebra\"). In subsequent decades, the distinction between semantic networks and knowledge graphs was blurred.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Some early knowledge graphs were topic-specific. In 1985, [Wordnet](https://en.wikipedia.org/wiki/Wordnet \"Wordnet\") was founded, capturing semantic relationships between words and meanings\\xa0– an application of this idea to language itself. In 2005, Marc Wirk founded [Geonames](https://en.wikipedia.org/wiki/Geonames \"Geonames\") to capture relationships between different geographic names and locales and associated entities. In 1998 Andrew Edmonds of Science in Finance Ltd in the UK created a system called ThinkBase that offered [fuzzy-logic](https://en.wikipedia.org/wiki/Fuzzy_logic \"Fuzzy logic\") based reasoning in a graphical context.[\\\\[6\\\\]](#cite_note-6)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='In 2007, both [DBpedia](https://en.wikipedia.org/wiki/DBpedia \"DBpedia\") and [Freebase](https://en.wikipedia.org/wiki/Freebase_\\\\(database\\\\) \"Freebase (database)\") were founded as graph-based knowledge [repositories](https://en.wikipedia.org/wiki/Repository_\\\\(version_control\\\\) \"Repository (version control)\") for general-purpose knowledge. DBpedia focused exclusively on data extracted from Wikipedia, while Freebase also included a range of public datasets. Neither described themselves as a \\'knowledge graph\\' but developed and described related concepts.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='In 2012, Google introduced their [Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_Graph \"Knowledge Graph\"),[\\\\[7\\\\]](#cite_note-Singhal-2012-7) building on DBpedia and Freebase among other sources. They later incorporated [RDFa](https://en.wikipedia.org/wiki/RDFa \"RDFa\"), [Microdata](https://en.wikipedia.org/wiki/Microdata_\\\\(HTML\\\\) \"Microdata (HTML)\"), [JSON-LD](https://en.wikipedia.org/wiki/JSON-LD \"JSON-LD\") content extracted from indexed web pages, including the _[CIA World Factbook](https://en.wikipedia.org/wiki/The_World_Factbook \"The World Factbook\")_, [Wikidata](https://en.wikipedia.org/wiki/Wikidata \"Wikidata\"), and [Wikipedia](https://en.wikipedia.org/wiki/Wikipedia \"Wikipedia\").[\\\\[7\\\\]](#cite_note-Singhal-2012-7)[\\\\[8\\\\]](#cite_note-8) Entity and', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='relationship types associated with this knowledge graph have been further organized using terms from the [schema.org](https://en.wikipedia.org/wiki/Schema.org \"Schema.org\")[\\\\[9\\\\]](#cite_note-McCusker-9) vocabulary. The Google Knowledge Graph became a successful complement to string-based search within Google, and its popularity online brought the term into more common use.[\\\\[9\\\\]](#cite_note-McCusker-9)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Since then, several large multinationals have advertised their knowledge graphs use, further popularising the term. These include Facebook, LinkedIn, [Airbnb](https://en.wikipedia.org/wiki/Airbnb \"Airbnb\"), [Microsoft](https://en.wikipedia.org/wiki/Microsoft \"Microsoft\"), [Amazon](https://en.wikipedia.org/wiki/Amazon.com \"Amazon.com\"), [Uber](https://en.wikipedia.org/wiki/Uber \"Uber\") and [eBay](https://en.wikipedia.org/wiki/EBay \"EBay\").[\\\\[10\\\\]](#cite_note-10)\\n\\nIn 2019, [IEEE](https://en.wikipedia.org/wiki/Institute_of_Electrical_and_Electronics_Engineers \"Institute of Electrical and Electronics Engineers\") combined its annual international conferences on \"Big Knowledge\" and \"Data Mining and Intelligent Computing\" into the International Conference on Knowledge Graph.[\\\\[11\\\\]](#cite_note-11)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Definitions\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=2 \"Edit section: Definitions\")\\\\]\\n-----------------------------------------------------------------------------------------------------------------------------------\\n\\nThere is no single commonly accepted definition of a knowledge graph. Most definitions view the topic through a Semantic Web lens and include these features:[\\\\[12\\\\]](#cite_note-12)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='*   _Flexible relations among knowledge in topical domains_: A knowledge graph (i) defines [abstract classes](https://en.wikipedia.org/wiki/Abstract_class \"Abstract class\") and relations of entities in a schema, (ii) mainly describes real world entities and their interrelations, organized in a graph, (iii) allows for potentially interrelating arbitrary entities with each other, and (iv) covers various topical domains.[\\\\[13\\\\]](#cite_note-13)\\n*   _General structure_: A network of entities, their semantic types, properties, and relationships.[\\\\[14\\\\]](#cite_note-14)[\\\\[15\\\\]](#cite_note-15) To represent properties, categorical or numerical values are often used.\\n*   _Supporting reasoning over inferred ontologies_: A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge.[\\\\[2\\\\]](#cite_note-Ref1-2)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='There are, however, many knowledge graph representations for which some of these features are not relevant. For those knowledge graphs, this simpler definition may be more useful:\\n\\n*   A digital structure that represents knowledge as concepts and the relationships between them (facts). A knowledge graph can include an ontology that allows both humans and machines to understand and reason about its contents.[\\\\[16\\\\]](#cite_note-16)[\\\\[17\\\\]](#cite_note-17)\\n\\n### Implementations\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=3 \"Edit section: Implementations\")\\\\]', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='In addition to the above examples, the term has been used to describe open knowledge projects such as [YAGO](https://en.wikipedia.org/wiki/YAGO_\\\\(database\\\\) \"YAGO (database)\") and Wikidata; federations like the Linked Open Data cloud;[\\\\[18\\\\]](#cite_note-18) a range of commercial search tools, including Yahoo\\'s semantic search assistant Spark, Google\\'s [Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_Graph \"Knowledge Graph\"), and Microsoft\\'s Satori; and the LinkedIn and Facebook entity graphs.[\\\\[2\\\\]](#cite_note-Ref1-2)\\n\\nThe term is also used in the context of [note-taking software](https://en.wikipedia.org/wiki/Note-taking_software \"Note-taking software\") applications that allow a user to build a [personal knowledge graph](https://en.wikipedia.org/wiki/Personal_knowledge_graph \"Personal knowledge graph\").[\\\\[19\\\\]](#cite_note-19)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='The popularization of knowledge graphs and their accompanying methods have led to the development of graph databases such as Neo4j[\\\\[20\\\\]](#cite_note-20) and GraphDB.[\\\\[21\\\\]](#cite_note-21) These graph databases allow users to easily store data as entities their interrelationships, and facilitate operations such as data reasoning, node embedding, and ontology development on knowledge bases.\\n\\nUsing a knowledge graph for reasoning over data\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=4 \"Edit section: Using a knowledge graph for reasoning over data\")\\\\]\\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='A knowledge graph formally represents semantics by describing entities and their relationships.[\\\\[22\\\\]](#cite_note-22) Knowledge graphs may make use of [ontologies](https://en.wikipedia.org/wiki/Ontology_\\\\(information_science\\\\) \"Ontology (information science)\") as a schema layer. By doing this, they allow [logical inference](https://en.wikipedia.org/wiki/Inference \"Inference\") for retrieving [implicit knowledge](https://en.wikipedia.org/wiki/Implicit_knowledge \"Implicit knowledge\") rather than only allowing queries requesting explicit knowledge.[\\\\[23\\\\]](#cite_note-23)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='In order to allow the use of knowledge graphs in various machine learning tasks, several methods for deriving latent feature representations of entities and relations have been devised. These knowledge graph embeddings allow them to be connected to machine learning methods that require feature vectors like [word embeddings](https://en.wikipedia.org/wiki/Word_embedding \"Word embedding\"). This can complement other estimates of conceptual similarity.[\\\\[24\\\\]](#cite_note-24)[\\\\[25\\\\]](#cite_note-25)[\\\\[26\\\\]](#cite_note-26)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Models for generating useful knowledge graph embeddings are commonly the domain of graph neural networks (GNNs).[\\\\[27\\\\]](#cite_note-27) GNNs are deep learning architectures that comprise edges and nodes, which correspond well to the entities and relationships of knowledge graphs. The topology and data structures afforded by GNNS provides a convenient domain for semi-supervised learning, wherein the network is trained to predict the value of a node embedding (provided a group of adjacent nodes and their edges) or edge (provided a pair of nodes). These tasks serve as fundamental abstractions for more complex tasks such as knowledge graph reasoning and alignment.[\\\\[28\\\\]](#cite_note-28)\\n\\n### Entity alignment\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=5 \"Edit section: Entity alignment\")\\\\]', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='[![Image 2](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Knowledge_graph_entity_alignment.png/440px-Knowledge_graph_entity_alignment.png)](https://en.wikipedia.org/wiki/File:Knowledge_graph_entity_alignment.png)\\n\\nTwo hypothetical knowledge graphs representing disparate topics contain a node that corresponds to the same entity in the real world. Entity alignment is the process of identifying such nodes across multiple graphs.\\n\\nAs new knowledge graphs are produced across a variety of fields and contexts, the same entity will inevitably be represented in multiple graphs. However, because no single standard for the construction or representation of knowledge graph exists, resolving which entities from disparate graphs correspond to the same real world subject is a non-trivial task. This task is known as _knowledge graph entity alignment_, and is an active area of research.[\\\\[29\\\\]](#cite_note-29)', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='Strategies for entity alignment generally seek to identify similar substructures, semantic relationships, shared attributes, or combinations of all three between two distinct knowledge graphs. Entity alignment methods use these structural similarities between generally non-isomorphic graphs to predict which nodes corresponds to the same entity.[\\\\[30\\\\]](#cite_note-30)\\n\\nThe recent successes of large language models (LLMs), in particular their effectiveness at producing syntactically meaningful embeddings, has spurred the use of LLMs in the task of entity alignment.[\\\\[31\\\\]](#cite_note-31)\\n\\nAs the amount of data stored in knowledge graphs grows, developing dependable methods for knowledge graph entity alignment becomes an increasingly crucial step in the integration and cohesion of knowledge graph data.\\n\\nSee also\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=6 \"Edit section: See also\")\\\\]\\n-----------------------------------------------------------------------------------------------------------------------------', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='*   [Concept map](https://en.wikipedia.org/wiki/Concept_map \"Concept map\")\\xa0– Diagram showing relationships among concepts\\n*   [Formal semantics (natural language)](https://en.wikipedia.org/wiki/Formal_semantics_\\\\(natural_language\\\\) \"Formal semantics (natural language)\")\\xa0– Study of meaning in natural languages\\n*   [Graph database](https://en.wikipedia.org/wiki/Graph_database \"Graph database\")\\xa0– Database that uses mathematical graphs to store and search data\\n*   [Knowledge graph embedding](https://en.wikipedia.org/wiki/Knowledge_graph_embedding \"Knowledge graph embedding\")\\xa0– Dimensionality reduction of graph-based semantic data objects \\\\[machine learning task\\\\]\\n*   [Logical graph](https://en.wikipedia.org/wiki/Logical_graph \"Logical graph\")\\xa0– Diagram of graphical syntax\\n*   [Semantic integration](https://en.wikipedia.org/wiki/Semantic_integration \"Semantic integration\")\\xa0– Interrelating info from diverse sources', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='*   [Semantic technology](https://en.wikipedia.org/wiki/Semantic_technology \"Semantic technology\")\\xa0– Technology to help machines understand data\\n*   [Topic map](https://en.wikipedia.org/wiki/Topic_map \"Topic map\")\\xa0– Knowledge organization system\\n*   [Vadalog](https://en.wikipedia.org/wiki/Vadalog \"Vadalog\")\\xa0– Type of Knowledge Graph Management System\\n*   [YAGO (database)](https://en.wikipedia.org/wiki/YAGO_\\\\(database\\\\) \"YAGO (database)\")\\xa0– Open-source information repository', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='References\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=7 \"Edit section: References\")\\\\]\\n---------------------------------------------------------------------------------------------------------------------------------', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='1.  **[^](#cite_ref-1 \"Jump up\")** [\"What is a Knowledge Graph?\"](https://ontotext.com/knowledgehub/fundamentals/what-is-a-knowledge-graph). 2018.\\n2.  ^ [Jump up to: _**a**_](#cite_ref-Ref1_2-0) [_**b**_](#cite_ref-Ref1_2-1) [_**c**_](#cite_ref-Ref1_2-2) Ehrlinger, Lisa; Wöß, Wolfram (2016). [_Towards a Definition of Knowledge Graphs_](http://ceur-ws.org/Vol-1695/paper4.pdf) (PDF). SEMANTiCS2016. Leipzig: Joint Proceedings of the Posters and Demos Track of 12th International Conference on Semantic Systems – SEMANTiCS2016 and 1st International Workshop on Semantic Change & Evolving Semantics (SuCCESS16). pp.\\xa013–16.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='3.  **[^](#cite_ref-3 \"Jump up\")** Soylu, Ahmet (2020). [\"Enhancing Public Procurement in the European Union Through Constructing and Exploiting an Integrated Knowledge Graph\"](https://doi.org/10.1007/978-3-030-62466-8_27). _The Semantic Web – ISWC 2020_. Lecture Notes in Computer Science. Vol.\\xa012507. pp.\\xa0430–446. [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1007/978-3-030-62466-8\\\\_27](https://doi.org/10.1007%2F978-3-030-62466-8_27). [ISBN](https://en.wikipedia.org/wiki/ISBN_\\\\(identifier\\\\) \"ISBN (identifier)\")\\xa0[978-3-030-62465-1](https://en.wikipedia.org/wiki/Special:BookSources/978-3-030-62465-1', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='\"Special:BookSources/978-3-030-62465-1\"). [S2CID](https://en.wikipedia.org/wiki/S2CID_\\\\(identifier\\\\) \"S2CID (identifier)\")\\xa0[226229398](https://api.semanticscholar.org/CorpusID:226229398).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='4.  **[^](#cite_ref-4 \"Jump up\")** Mohamed, Sameh K.; Nounu, Aayah; Nováček, Vít (2021). [\"Biological applications of knowledge graph embedding models\"](https://doi.org/10.1093%2Fbib%2Fbbaa012). _Briefings in Bioinformatics_. **22** (2): 1679–1693. [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1093/bib/bbaa012](https://doi.org/10.1093%2Fbib%2Fbbaa012). [hdl](https://en.wikipedia.org/wiki/Hdl_\\\\(identifier\\\\) \"Hdl', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='(identifier)\"):[1983/919db5c6-6e10-4277-9ff9-f86bbcedcee8](https://hdl.handle.net/1983%2F919db5c6-6e10-4277-9ff9-f86bbcedcee8). [PMID](https://en.wikipedia.org/wiki/PMID_\\\\(identifier\\\\) \"PMID (identifier)\")\\xa0[32065227](https://pubmed.ncbi.nlm.nih.gov/32065227) – via Oxford Academic.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='5.  **[^](#cite_ref-5 \"Jump up\")** Edward W. Schneider. 1973. Course Modularization Applied: The Interface System and Its Implications For Sequence Control and Data Analysis. In Association for the Development of Instructional Systems (ADIS), Chicago, Illinois, April 1972\\n6.  **[^](#cite_ref-6 \"Jump up\")** [\"US Trademark no 75589756\"](http://tmsearch.uspto.gov/bin/showfield?f=doc&state=4809:rjqm9h.2.1).\\n7.  ^ [Jump up to: _**a**_](#cite_ref-Singhal-2012_7-0) [_**b**_](#cite_ref-Singhal-2012_7-1) Singhal, Amit (May 16, 2012). [\"Introducing the Knowledge Graph: things, not strings\"](https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html). _Official Google Blog_. Retrieved 21 March 2017.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='8.  **[^](#cite_ref-8 \"Jump up\")** Schwartz, Barry (December 17, 2014). [\"Google\\'s Freebase To Close After Migrating To Wikidata: Knowledge Graph Impact?\"](https://www.seroundtable.com/google-freebase-wikidata-knowledge-graph-19591.html). _[Search Engine Roundtable](https://en.wikipedia.org/wiki/Search_Engine_Roundtable \"Search Engine Roundtable\")_. Retrieved December 10, 2017.\\n9.  ^ [Jump up to: _**a**_](#cite_ref-McCusker_9-0) [_**b**_](#cite_ref-McCusker_9-1) McCusker, James P.; McGuiness, Deborah L. [\"What is a Knowledge Graph?\"](https://www.authorea.com/users/6341/articles/107281-what-is-a-knowledge-graph/_show_article). _www.authorea.com_. Retrieved 21 March 2017.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='10.  **[^](#cite_ref-10 \"Jump up\")** [\"Knowledge Graph Enterprises\"](https://kgkg.factnexus.com/@3782~167.html). 2020.\\n11.  **[^](#cite_ref-11 \"Jump up\")** [\"2021 IEEE International Conference on Knowledge Graph (ICKG)\\\\*\"](https://kmeducationhub.de/ieee-international-conference-big-knowledge-icbk/). _KMedu Hub_. 2017-07-09. Retrieved 2021-03-22.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='12.  **[^](#cite_ref-12 \"Jump up\")** Hogan, Aidan; Blomqvist, Eva; Cochez, Michael; d\\'Amato, Claudia; de Melo, Gerard; Gutierrez, Claudio; Labra Gayo, José Emilio; Kirrane, Sabrina; Neumaier, Sebastian; Polleres, Axel; Navigli, Roberto; Ngonga Ngomo, Axel-Cyrille; Rashid, Sabbir M.; Rula, Anisa; Schmelzeisen, Lukas; Sequeda, Juan; Staab, Steffen; Zimmermann, Antoine (2021-01-24). \"Knowledge Graphs\". _ACM Computing Surveys_. **54** (4): 1–37. [arXiv](https://en.wikipedia.org/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv (identifier)\"):[2003.02320](https://arxiv.org/abs/2003.02320). [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='(identifier)\"):[10.1145/3447772](https://doi.org/10.1145%2F3447772). [ISSN](https://en.wikipedia.org/wiki/ISSN_\\\\(identifier\\\\) \"ISSN (identifier)\")\\xa0[0360-0300](https://www.worldcat.org/issn/0360-0300). [S2CID](https://en.wikipedia.org/wiki/S2CID_\\\\(identifier\\\\) \"S2CID (identifier)\")\\xa0[235716181](https://api.semanticscholar.org/CorpusID:235716181).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='13.  **[^](#cite_ref-13 \"Jump up\")** Paulheim, Heiko (2017). [\"Knowledge Graph Refinement: A Survey of Approaches and Evaluation Methods\"](http://www.semantic-web-journal.net/system/files/swj1083.pdf) (PDF). _Semantic Web_: 489–508. Retrieved 21 March 2017.\\n14.  **[^](#cite_ref-14 \"Jump up\")** Krötsch, Markus; Weikum, Gerhard (March 2016). [\"Editorial of the Special Issue on Knowledge Graphs\"](https://doi.org/10.1016/j.websem.2016.04.002). _Journal of Web Semantics_. 37–38: 53–54. [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1016/j.websem.2016.04.002](https://doi.org/10.1016%2Fj.websem.2016.04.002). Retrieved 10 February 2021.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='15.  **[^](#cite_ref-15 \"Jump up\")** [\"What is a Knowledge Graph?|Ontotext\"](https://www.ontotext.com/knowledgehub/fundamentals/what-is-a-knowledge-graph). _Ontotext_. Retrieved 2020-07-01.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='16.  **[^](#cite_ref-16 \"Jump up\")** Peng, Ciyuan; Feng, Xia; Naseriparsa, Mehdi; Osborne, Francesco (2023). [\"Knowledge Graphs: Opportunities and Challenges\"](https://doi.org/10.1007/s10462-023-10465-9). _Artificial Intelligence Review_. **56** (11): 13071–13102. [arXiv](https://en.wikipedia.org/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv (identifier)\"):[2303.13948](https://arxiv.org/abs/2303.13948). [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1007/s10462-023-10465-9](https://doi.org/10.1007%2Fs10462-023-10465-9). [ISSN](https://en.wikipedia.org/wiki/ISSN_\\\\(identifier\\\\) \"ISSN', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='(identifier)\")\\xa0[1573-7462](https://www.worldcat.org/issn/1573-7462). [PMC](https://en.wikipedia.org/wiki/PMC_\\\\(identifier\\\\) \"PMC (identifier)\")\\xa0[10068207](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068207). [PMID](https://en.wikipedia.org/wiki/PMID_\\\\(identifier\\\\) \"PMID (identifier)\")\\xa0[37362886](https://pubmed.ncbi.nlm.nih.gov/37362886).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='17.  **[^](#cite_ref-17 \"Jump up\")** [\"The Knowledge Graph about Knowledge Graphs\"](https://kgkg.factnexus.com/@3782~6.html). 2020.\\n18.  **[^](#cite_ref-18 \"Jump up\")** [\"The Linked Open Data Cloud\"](https://lod-cloud.net/). _lod-cloud.net_. Retrieved 2020-06-30.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='19.  **[^](#cite_ref-19 \"Jump up\")** Pyne, Yvette; Stewart, Stuart (March 2022). [\"Meta-work: how we research is as important as what we research\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8884432). _[British Journal of General Practice](https://en.wikipedia.org/wiki/British_Journal_of_General_Practice \"British Journal of General Practice\")_. **72** (716): 130–131. [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.3399/bjgp22X718757](https://doi.org/10.3399%2Fbjgp22X718757). [PMC](https://en.wikipedia.org/wiki/PMC_\\\\(identifier\\\\) \"PMC (identifier)\")\\xa0[8884432](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8884432).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='[PMID](https://en.wikipedia.org/wiki/PMID_\\\\(identifier\\\\) \"PMID (identifier)\")\\xa0[35210247](https://pubmed.ncbi.nlm.nih.gov/35210247).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='20.  **[^](#cite_ref-20 \"Jump up\")** [\"Neo4j Graph Database & Analytics | Graph Database Management System\"](https://neo4j.com/). _Neo4j_. Retrieved 8 November 2023.\\n21.  **[^](#cite_ref-21 \"Jump up\")** [\"Ontotext GraphDB\"](https://www.ontotext.com/products/graphdb/). _Ontotext_. Retrieved 8 November 2023.\\n22.  **[^](#cite_ref-22 \"Jump up\")** [\"How do knowledge graphs work?\"](https://www.stardog.com/knowledge-graph/). _Stardog_. 2022-04-05. Retrieved 2022-04-05.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='23.  **[^](#cite_ref-23 \"Jump up\")** [\"Unlocking the Power of Google Knowledge Panel: How to Obtain and Claim Yours in 2023 – RH Razu\"](https://rhrazu.com/google-knowledge-panel-obtain-and-claim-yours-in-2023/). _rhrazu.com_. 2023-09-01. Retrieved 2023-09-05.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='24.  **[^](#cite_ref-24 \"Jump up\")** Hongwei Wang (October 2018). \"RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems\". _Proceedings of the 27th ACM International Conference on Information and Knowledge Management_. pp.\\xa0417–426. [arXiv](https://en.wikipedia.org/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv (identifier)\"):[1803.03467](https://arxiv.org/abs/1803.03467). [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1145/3269206.3271739](https://doi.org/10.1145%2F3269206.3271739). [ISBN](https://en.wikipedia.org/wiki/ISBN_\\\\(identifier\\\\) \"ISBN (identifier)\")\\xa0[9781450360142](https://en.wikipedia.org/wiki/Special:BookSources/9781450360142', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='\"Special:BookSources/9781450360142\"). [S2CID](https://en.wikipedia.org/wiki/S2CID_\\\\(identifier\\\\) \"S2CID (identifier)\")\\xa0[3766110](https://api.semanticscholar.org/CorpusID:3766110).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='25.  **[^](#cite_ref-25 \"Jump up\")** [\"Embedding models for knowledge graph completion\"](https://towardsdatascience.com/embedding-models-for-knowledge-graph-completion-a66d4c01d588?sk=d86f99fec0e7ec87c352b3d0f5b80915). 19 July 2020.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='26.  **[^](#cite_ref-26 \"Jump up\")** Ristoski, Petar; Paulheim, Heiko (2016), [\"RDF2Vec: RDF Graph Embeddings for Data Mining\"](https://madoc.bib.uni-mannheim.de/41307/1/Ristoski_RDF2Vec.pdf) (PDF), _The Semantic Web – ISWC 2016_, Lecture Notes in Computer Science, vol.\\xa09981, pp.\\xa0498–514, [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1007/978-3-319-46523-4\\\\_30](https://doi.org/10.1007%2F978-3-319-46523-4_30), [ISBN](https://en.wikipedia.org/wiki/ISBN_\\\\(identifier\\\\) \"ISBN', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='(identifier)\")\\xa0[978-3-319-46522-7](https://en.wikipedia.org/wiki/Special:BookSources/978-3-319-46522-7 \"Special:BookSources/978-3-319-46522-7\")', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='27.  **[^](#cite_ref-27 \"Jump up\")** Zhou, Jie; et\\xa0al. (2020). [\"Graph neural networks: A review of methods and applications\"](https://doi.org/10.1016%2Fj.aiopen.2021.01.001). _AI Open_. **1** (1): 57–81. [arXiv](https://en.wikipedia.org/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv (identifier)\"):[1812.08434](https://arxiv.org/abs/1812.08434). [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1016/j.aiopen.2021.01.001](https://doi.org/10.1016%2Fj.aiopen.2021.01.001). [S2CID](https://en.wikipedia.org/wiki/S2CID_\\\\(identifier\\\\) \"S2CID', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='(identifier)\")\\xa0[56517517](https://api.semanticscholar.org/CorpusID:56517517) – via Elsevier Science Direct.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='28.  **[^](#cite_ref-28 \"Jump up\")** Ye, Zi; Kumar, Yogan Jaya; Sing, Goh Ong; Song, Fengyan; Wang, Junsong (2022). [\"A comprehensive survey of graph neural networks for knowledge graphs\"](https://doi.org/10.1109%2FACCESS.2022.3191784). _IEEE Access_. **10**: 75729–7574. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_\\\\(identifier\\\\) \"Bibcode (identifier)\"):[2022IEEEA..1075729Y](https://ui.adsabs.harvard.edu/abs/2022IEEEA..1075729Y). [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1109/ACCESS.2022.3191784](https://doi.org/10.1109%2FACCESS.2022.3191784).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='[S2CID](https://en.wikipedia.org/wiki/S2CID_\\\\(identifier\\\\) \"S2CID (identifier)\")\\xa0[250654689](https://api.semanticscholar.org/CorpusID:250654689) – via IEEE Xplore.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='29.  **[^](#cite_ref-29 \"Jump up\")** Berrendorf, Max; Faerman, Evgeniy; Melnychuk, Valentyn; Tresp, Volker; Seidl, Thomas (April 14–17, 2020). _Knowledge graph entity alignment with graph convolutional networks: lessons learned_. Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal. Lecture Notes in Computer Science. Vol.\\xa0Proceedings, Part II. pp.\\xa03–11. [arXiv](https://en.wikipedia.org/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv (identifier)\"):[1911.08342](https://arxiv.org/abs/1911.08342). [doi](https://en.wikipedia.org/wiki/Doi_\\\\(identifier\\\\) \"Doi (identifier)\"):[10.1007/978-3-030-45442-5\\\\_1](https://doi.org/10.1007%2F978-3-030-45442-5_1).', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='[ISBN](https://en.wikipedia.org/wiki/ISBN_\\\\(identifier\\\\) \"ISBN (identifier)\")\\xa0[978-3-030-45441-8](https://en.wikipedia.org/wiki/Special:BookSources/978-3-030-45441-8 \"Special:BookSources/978-3-030-45441-8\"). [S2CID](https://en.wikipedia.org/wiki/S2CID_\\\\(identifier\\\\) \"S2CID (identifier)\")\\xa0[208158314](https://api.semanticscholar.org/CorpusID:208158314) – via Springer International Publishing.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='30.  **[^](#cite_ref-30 \"Jump up\")** Chaurasiya, Deepak; Surisetty, Anil; Kumar, Nitish; Singh, Alok; Dey, Vikrant; Malhotra, Aakarsh; Dhama, Gaurav; Arora, Ankur (2022). \"Entity alignment for knowledge graphs: progress, challenges, and empirical studies\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\\\\(identifier\\\\) \"ArXiv (identifier)\"):[2205.08777](https://arxiv.org/abs/2205.08777) \\\\[[cs.AI](https://arxiv.org/archive/cs.AI)\\\\].', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='31.  **[^](#cite_ref-31 \"Jump up\")** Hogan, Aidan; Lippolis, Anna Sofia; Klironomos, Antonis; Milon-Flores, Daniela F.; Zheng, Heng; Jouglar, Alexane; Norouzi, Ebrahim (2023). [\"Enhancing Entity Alignment Between Wikidata and ArtGraph using LLMs\"](https://aidanhogan.com/docs/art_wikidata_kgs_llms.pdf) (PDF). _Proceedings of the International Workshop on Semantic Web and Ontology Design for Cultural Heritage_ – via International Workshop on Semantic Web and Ontology Design for Cultural Heritage (SWODCH), Athens, Greece.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'}), Document(page_content='External links\\\\[[edit](https://en.wikipedia.org/w/index.php?title=Knowledge_graph&action=edit&section=8 \"Edit section: External links\")\\\\]\\n-----------------------------------------------------------------------------------------------------------------------------------------\\n\\n*   Will Douglas Heaven (4 September 2020). [\"This know-it-all AI learns by reading the entire web nonstop\"](https://www.technologyreview.com/2020/09/04/1008156/knowledge-graph-ai-reads-web-machine-learning-natural-language-processing/). _MIT Technology Review_. Retrieved 5 September 2020. Diffbot is building the biggest-ever knowledge graph by applying image recognition and natural-language processing to billions of web pages.', metadata={'title': 'Knowledge graph', 'url': 'https://en.wikipedia.org/wiki/Knowledge_graph', 'publishedTime': '2020-06-29T23:24:06Z'})]\n"
     ]
    }
   ],
   "source": [
    "#split document into smaller chunks. Smaller chunk sizes are usually better (not too small) but your results will vary depending on the prompt and your local data. Will take longer to index if chunk size is small but will potentially alleviate loss-in-the-middle issues.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter().from_tiktoken_encoder(\n",
    "    chunk_size=256, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(texts=[doc['content'] for doc in docs_list], metadatas=[doc['metadata'] for doc in docs_list])\n",
    "\n",
    "\n",
    "# filter out metadata that comes as an array and restrict it to just primitive types\n",
    "filtered_docs = []\n",
    "\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        if doc.metadata is not None:\n",
    "            clean_metadata = {k: v for k, v in doc.metadata.items() if type(v) in [str, int, float, bool]}\n",
    "        else:\n",
    "            clean_metadata = {}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
