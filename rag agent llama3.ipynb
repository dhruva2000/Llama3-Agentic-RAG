{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain-nomic langchain_community tiktoken chromadb langchainhub langchain langgraph tavily-python gpt4all firecrawl-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load our .env file. If you don't have one, create it and include a langchain api key, jina key and tavily search key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Take environment variables from .env.\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANG_KEY') # replace with your own key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = 'llama3' # Change this to a model of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I am using a bunch of webURLs as my primary data source. You can try this out by loading a CSV file with your own data or maybe even a database with company information, a JSON file with some semantic encoding information etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "import requests\n",
    "\n",
    "# Bunch of randomly generated URLs (restricting to 3 to not destroy API token limits)\n",
    "urls = {\n",
    "    'https://en.wikipedia.org/wiki/Knowledge_graph',\n",
    "    'https://en.wikipedia.org/wiki/Semantic_technology',\n",
    "    'https://en.wikipedia.org/wiki/Semantic_integration'\n",
    "    # 'https://en.wikipedia.org/wiki/Logical_graph',\n",
    "    # 'https://en.wikipedia.org/wiki/Knowledge_graph_embedding',\n",
    "    # 'https://en.wikipedia.org/wiki/Graph_database',\n",
    "    # 'https://en.wikipedia.org/wiki/Formal_semantics_(natural_language)',\n",
    "    # 'https://en.wikipedia.org/wiki/Artificial_general_intelligence',\n",
    "    # 'https://en.wikipedia.org/wiki/Recursive_self-improvement',\n",
    "    # 'https://en.wikipedia.org/wiki/Automated_planning_and_scheduling',\n",
    "    # 'https://en.wikipedia.org/wiki/Machine_learning',\n",
    "    # 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "   'Accept': 'application/json',\n",
    "   'Authorization': os.getenv('JINA_KEY') # replace with your own api key \n",
    "}\n",
    "\n",
    "base_url = 'https://r.jina.ai/'\n",
    "\n",
    "docs = [requests.get(base_url+url, headers=headers).json() for url in urls]\n",
    "\n",
    "docs_list = []\n",
    "\n",
    "# Look up JINA API response format but essentially we are extracting the content and reconstructing metadata from the response\n",
    "for doc in docs:\n",
    "    metadata = {k: v for k, v in doc['data'].items() if k != 'content'}\n",
    "    docs_list.append({\"content\": doc['data']['content'], \"metadata\": metadata})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document into smaller chunks. Smaller chunk sizes are usually better for tasks where you want to extract more granular information or meaning from individual words e.g. SEO or grammar/syntax checking. For a more holistic understanding of your data, use a larger chunk size. Chunk overlap is the number of characters that will be shared between adjacent chunks. This is useful for tasks where you want to maintain context between chunks e.g. sentiment analysis or topic modeling.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter().from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=5\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(texts=[doc['content'] for doc in docs_list], metadatas=[doc['metadata'] for doc in docs_list])\n",
    "\n",
    "\n",
    "# Filter out metadata that comes as an array because that isn't supported\n",
    "filtered_docs = []\n",
    "\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        if doc.metadata is not None:\n",
    "            clean_metadata = {k: v for k, v in doc.metadata.items() if type(v) in [str, int, float, bool]}\n",
    "        else:\n",
    "            clean_metadata = {}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vector DB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding = GPT4AllEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", #this is a smaller embedding model for test purposes. make sure your chunk size doesnt exceed the model's context length (in this mode, the max context length is 512)\n",
    "        gpt4all_kwargs={'allow_download': 'True'}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a retriever from our vectorstore\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a **retrieval grader** to determine if the document pulled is relevant to the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatOllama(model=local_llm, format='json', temperature=0)\n",
    "\n",
    "# The following prompt is generally how you would structure a retrieval grader prompt with roles defined between header_ids\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. If the document contains topics/concepts/keywords related\n",
    "    to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "    \\n Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score'.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "# # Chain these steps together\n",
    "# retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "# question_right = \"What is a knowledge graph?\" # Test question to see if it can identify a relevant document from our store\n",
    "# docs_right = retriever.invoke(question_right)\n",
    "# doc_text = docs_right[1].page_content # We take a sample document from the retrieved documents\n",
    "# print(retrieval_grader.invoke({\"question\": question_right, \"document\": doc_text})) # Now we check to see if that doc is relevant\n",
    "\n",
    "# question_wrong = \"Who made sesame street?\"\n",
    "# docs_wrong = retriever.invoke(question_wrong)\n",
    "# doc_text = docs_wrong[1].page_content\n",
    "# print(retrieval_grader.invoke({\"question\": question_wrong, \"document\": doc_text})) # Now we check to see if that doc is relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now handle response generation with the document that was retrieved by defining a **rag_chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant tasked with generating a response to a user question. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use a maximum of 3 sentences and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is my question: {question} \n",
    "    Here is the potential context: {context}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the chat model this time you don't need json as you want a string output\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Chain the steps together\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# # Run\n",
    "# question = \"Who made sesame street?\"\n",
    "# docs = retriever.invoke(question)\n",
    "# result = rag_chain.invoke({\"question\": question, \"context\": docs})\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the event that the retrieved document is not relevant, let's opt for a web search via Tavily via a **web_search_tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# The api key is already set as TAVILY_API_KEY in the .env file and will be automatically pulled\n",
    "web_search_tool = TavilySearchResults(maxResults=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine if the output was a hallucination or not by creating another **hallucination_grader** agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM instantiation\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# This agent will assume the responsibilty of checking for hallucinations (i.e. is the response grounded in facts)\n",
    "prompt = PromptTemplate(\n",
    "    template='''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a grader assessing the quality of a generated response. If the response is based in facts relevant to the question, grade it as relevant. If the response is incoherent or irrelevant, grade it as irrelevant. The goal is to filter out erroneous or hallucinating responses. \\n Give a binary score of 'yes' or 'no' to indicate whether the response is grounded in truths and known facts. Your output should be a key of 'score' and the binary score you have determined.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the question: \\n {question} \\n\n",
    "    Here is the generated response: \\n {response} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>''',\n",
    "    input_variables=[\"question\", \"response\"]\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "# hallucination_grader.invoke({\"question\": question, \"response\": result}) #using values from the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an **answer_grader** agent to determine how good our response was for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM instantiation\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# This agent will assume the responsibilty of checking how useful the answer was\n",
    "prompt = PromptTemplate(\n",
    "    template='''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a grader assessing the quality of a generated response. If the response is coherent and relevant to the question, grade it as relevant. If the response is incoherent or irrelevant, grade it as irrelevant. The goal is to filter out responses that don't actually answer the question well. \\n Give a binary score of 'yes' or 'no' to indicate whether the response is relevant to the question. Your output should be a key of 'score' and the binary score you have determined.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the question: \\n {question} \\n\n",
    "    Here is the generated response: \\n {response} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>''',\n",
    "    input_variables=[\"question\", \"response\"]\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "# answer_grader.invoke({\"question\": question, \"response\": result}) #using values from the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we chain everything together by defining states and nodes using lang graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define the global state of our graph\n",
    "class MyState(TypedDict):\n",
    "    question: str  # the question that the user asked\n",
    "    generation: str  # the output at each step\n",
    "    web_search: bool  # whether a web_search was conducted\n",
    "    documents: List[Document]  # list of documents currently valid\n",
    "\n",
    "# Define nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from the vectorstore\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    Returns:\n",
    "        state(dict): The updated state of the graph with the retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state['question']\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"question\": question, \"documents\": documents}  # Updating the global documents state here\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether retrieved documents are relevant to the question. If any document is not relevant it will switch the flag of web_search to True\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    Returns:\n",
    "        state(dict): Filters out irrelevant documents and updates web_search state\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    filtered_docs = []\n",
    "    web_search = False\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        grade = score['score']\n",
    "        if grade.lower() == 'yes':\n",
    "            print(\"---DOCUMENT IS RELEVANT---\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"---DOCUMENT IS NOT RELEVANT---\")\n",
    "            web_search = True\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}  # Once again updating the global state here\n",
    "\n",
    "def generate_rag(state):\n",
    "    \"\"\"\n",
    "    Generate answers using RAG on retrieved documents\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    Returns:\n",
    "        state(dict): Adds a new key to the state, generation, which contains the generated LLM response\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    # RAG Generation\n",
    "    generation = rag_chain.invoke({\"question\": question, \"context\": documents})\n",
    "    print(f\"Generated response: {generation}\")  # Debugging line\n",
    "\n",
    "    return {\"question\": question, \"documents\": documents, \"generation\": generation}\n",
    "\n",
    "def perform_web_search(state):\n",
    "    \"\"\"\n",
    "    Conducts a web search using Tavily\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    Returns:\n",
    "        state(dict): Adds the web search results to the documents\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    # Web search generation\n",
    "    docs = web_search_tool.invoke({'query': question})\n",
    "    web_results = \"\\n\".join([d['content'] for d in docs])\n",
    "    web_results_doc = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results_doc)\n",
    "    else:\n",
    "        documents = [web_results_doc]\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# Conditional edge\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Decides whether to generate an answer or conduct a web search\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    Returns:\n",
    "        str: binary decision for next node to be called\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_docs = state[\"documents\"]\n",
    "\n",
    "    if web_search:\n",
    "        print(\"---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO THE QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"perform_web_search\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "# Conditional edge 2\n",
    "def check_hallucination(state):\n",
    "    \"\"\"\n",
    "    Checks if the generated response is coherent\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    Returns:\n",
    "        str: binary decision for next node to be called\n",
    "    \"\"\"\n",
    "    print(\"---CHECK HALLUCINATION---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    score = hallucination_grader.invoke({\"question\": question, \"response\": generation})\n",
    "    grade = score['score']\n",
    "    if grade.lower() == 'yes':\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN RELEVANT DOCUMENTS---\")\n",
    "        print(\"---GRADE GENERATION vs QUESTION ---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"response\": generation})\n",
    "        grade = score['score']\n",
    "        if grade.lower() == 'yes':\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN FACTS...RETRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "# Initialize graph and nodes\n",
    "workflow = StateGraph(MyState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate_rag)\n",
    "workflow.add_node(\"perform_web_search\", perform_web_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, let's connect our conditional edges as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Build the graph ----------------------------- #\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"perform_web_search\": \"perform_web_search\",\n",
    "        \"generate\": \"generate\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"perform_web_search\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    check_hallucination,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"perform_web_search\",\n",
    "        \"not supported\": \"generate\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n",
    "\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"How can I create a knowledge graph from scratch?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for k, v in output.items():\n",
    "        print(f\"Finished running: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a knowledge graph from scratch, you can follow these general steps:\n",
      "\n",
      "1. Define the scope and purpose of your knowledge graph, including the types of entities and relationships you want to include.\n",
      "2. Design an ontology or schema for your knowledge graph, which will define the structure and vocabulary used to represent your data.\n",
      "3. Collect and prepare your data, such as extracting information from various sources like databases, documents, or web pages.\n",
      "4. Use a graph database like Neo4j or GraphDB to store your data as entities and their interrelationships.\n",
      "5. Implement methods for reasoning over your data, such as node embedding and ontology development.\n",
      "\n",
      "Note that the specific steps may vary depending on the size and complexity of your knowledge graph, as well as the tools and technologies you choose to use.\n"
     ]
    }
   ],
   "source": [
    "# Get  the value corresponding to 'generation' in the final output\n",
    "\n",
    "final = list(output.values())\n",
    "for k, v in final[0].items():\n",
    "    if k == 'generation':\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
